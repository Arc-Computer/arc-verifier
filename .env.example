# Arc-Verifier Environment Configuration
# Copy this file to .env and fill in your API keys

# LLM Provider API Keys (choose your preferred provider)
# =====================================================

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration  
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Cohere Configuration (optional)
COHERE_API_KEY=your_cohere_api_key_here

# LLM Settings
# ============

# Primary LLM provider for agent evaluation
# Options: "openai", "anthropic", "cohere", "local"
LLM_PRIMARY_PROVIDER=anthropic

# Fallback LLM provider for ensemble evaluation
# Options: "openai", "anthropic", "cohere", "local", or leave empty to disable
LLM_FALLBACK_PROVIDER=openai

# Enable ensemble evaluation (uses both primary and fallback)
LLM_ENABLE_ENSEMBLE=true

# LLM request timeout in seconds
LLM_TIMEOUT_SECONDS=30

# Maximum tokens for LLM responses
LLM_MAX_TOKENS=2048

# Performance and Scaling
# =======================

# Docker daemon timeout (seconds)
DOCKER_TIMEOUT=30

# Benchmark duration for quick scans (seconds)
BENCHMARK_QUICK_DURATION=15

# Benchmark duration for full verification (seconds)
BENCHMARK_FULL_DURATION=60

# Advanced Settings
# =================

# Enable debug logging
DEBUG=false

# Cache LLM responses (for development/testing)
LLM_CACHE_RESPONSES=false

# Skip LLM calls in CI/CD environments
CI_SKIP_LLM=false

# Custom LLM endpoint for local models
LOCAL_LLM_ENDPOINT=http://localhost:8000/v1

# TEE Validation Settings
# =======================

# Skip TEE validation for development
SKIP_TEE_VALIDATION=false

# TEE attestation timeout (seconds)
TEE_TIMEOUT=10

# Trivy Scanner Settings
# ======================

# Trivy database update interval (hours)
TRIVY_UPDATE_INTERVAL=24

# Trivy cache directory
TRIVY_CACHE_DIR=.trivy-cache

# Example Production Configuration for NEAR Protocol
# =================================================
# 
# For production deployment with NEAR Protocol:
# 
# LLM_PRIMARY_PROVIDER=anthropic
# LLM_FALLBACK_PROVIDER=openai
# LLM_ENABLE_ENSEMBLE=true
# DOCKER_TIMEOUT=60
# BENCHMARK_FULL_DURATION=120
# DEBUG=false
# CI_SKIP_LLM=false
# SKIP_TEE_VALIDATION=false